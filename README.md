# RAG Chatbot - For Video Game History 

## Introduction

This project showcases a Retrieval-Augmented Generation chatbot built with the LangChain framework and Streamlit UI.

It answers natural-language questions about 337 video games from an HTML table by retrieving relevant context using a FAISS index and similarity search. Game data is embedded and stored locally for fast retrieval. The chatbot uses OpenAI's API to generate accurate responses, and users can reference best_video_games.csv while chatting. 

Read the How to Run section for step-by-step setup instructions on your Mac.

To get started, make sure you add a valid OpenAI API key to config.py

## Data Sources

To support analysis described above, this project used 3 data sources:

| # | Name/short description | Source URL | Type: API/Webpage file | List of fields | Format | Estimated data size, number of data points planned to use |
|---|------------------------|------------|-------------------------|----------------|--------|------------------------------------------------------------|
| 1 | List of video games considered the best (337 games from 1970 ~ 2023) | https://en.wikipedia.org/wiki/List_of_video_games_considered_the_best | Wikipedia HTML table | Game Production Year, Game Title, Genre, Publisher, Original Platforms | HTML text | 337 rows and 5 valid columns |
| 2 | A .txt file contains all 337 games’ Wikipedia descriptions | Generated by codes | Collected from Wikipedia HTML pages and written into .txt | Comprehensive description of each game in great details (game intro, contents, history, popularity, etc.) | .txt | 9MB, contains over 290K English words |
| 3 | A structured CSV file contains cleaned table from source 1 with game publisher country info | Generated by codes | Wikipedia HTML table transformed into CSV | Game Production Year, Game Title, Genre, Publisher, Original Platforms, HQ Location of Publisher | CSV | 337 rows and 6 valid columns |

## RAG Chatbot

This RAG chatbot processes a .txt file containing Wikipedia descriptions of all 337 games and allows users to ask questions about any game. The chatbot uses vector embeddings and similarity search to identify the most relevant text chunks, then generates human-readable answers powered by GPT-4. This system enables flexible content analysis—for example, if a user wants to know why Minecraft is popular, they can simply ask the chatbot and receive a professional response.

The RAG system designed for this project follows the workflow below:
1. Chunking the .txt file into manageable segments.
2. Extracting keywords from each chunk using KeyBERT.
3. Embedding each chunk and its keywords into vectors.
4. Storing the vectors in a FAISS (Facebook AI Similarity Search) index file.
5. Accepting user questions through a UI.
6. Performing similarity search to retrieve the most relevant chunks and keywords.
7. Sending the retrieved content and the user’s question to GPT-4 via a custom AI prompt.
8. Generating and returning a natural-language answer to the user.

## RAG Chatbot Results

For example, when asked:  
**“Introduce the Tetris to me. Is this game popular? If so, why do people like it?”**

The chatbot responded with a well-structured summary, noting that Tetris was created in 1985 by Soviet engineer Alexey Pajitnov, explaining its gameplay, and highlighting its widespread appeal due to simplicity and cultural impact—especially after Nintendo's global promotion.

This shows the chatbot’s ability to understand nuanced questions, retrieve the right content, and deliver clear, factual answers in real time.

While designed for video game analysis, the system is highly flexible. With a different dataset, it can be applied to:

- Customer service, to answer product or support questions
- Education, to help users explore course content interactively
- Internal knowledge tools, for fast document or policy lookup
- Healthcare, to provide medically-informed answers to common patient questions

In short, this RAG chatbot framework can power intelligent, conversational systems across a wide range of industries.


## How to Run

This project includes both visual analysis and a RAG chatbot based on Wikipedia data for 337 highly rated video games. Below are the steps to reproduce the entire pipeline and interact with the chatbot.

### 1. Downloading Code zip

First, go to this repository’s main page -> Click the green "Code" buttom -> Click Download ZIP

### 2. Get your own OpenAI API KEY!

Get your OpenAI API Key at: https://platform.openai.com/api-keys
Then copy your API Key into config.py in downloaded src/ folder

### 3. Create and activate the conda environment

You MUST run this in Conda environment, this means that every time you try to re-run this project after you closed your terminal,
you have to follow the steps below again.

Open your terminal, cd to the downloaded file's directory DSCI510_Final_Project
then run command below in your terminal:

```bash
conda create -n rag_env python=3.9 -y
conda activate rag_env
```

### 4. Upgrade pip and essential build tools

```bash
pip install --upgrade pip setuptools wheel
```

### 5. Install core packages via conda

```bash
conda install -c conda-forge faiss-cpu pyarrow -y
```

### 6. Install Python dependencies

```bash
pip install -r requirements.txt
```

### Then open jupyter notebook, open results.ipynb

Now run the first block in jupyter notebook:

```bash
import sys
import os
sys.path.append(os.path.join(os.getcwd(), 'src'))
!pip install -r requirements.txt
```

You might need to restart the notebook after running this block.

### 1. Data Collection and Cleaning

This step scrapes a Wikipedia table of video games and follows publisher hyperlinks to extract their headquarters locations. The result is a cleaned CSV file, which can be used as a reference when you're not sure what to ask the chatbot.

```python
from get_table_HQ import get_best_games_table
get_best_games_table()
```


### 2. Text Corpus Scraping (game descriptions)

This step scrapes the Wikipedia pages for each game and saves them into a single `.txt` file.

```python
from hyperlink_scraper import extract_game_links_and_genres, save_to_txt
games = extract_game_links_and_genres(limit=337)
save_to_txt(games, output_file="game.txt")
```

### 3. Chunking, Tagging, and Indexing

Split the text file into chunks, extract tags with KeyBERT, and store everything in an SQLite database and FAISS vector index.

Make sure you already put your OpenAI API key in src/config.py

Then run:

```python
from rag import (
    detect_file_type,
    extract_text,
    split_text_into_chunks,
    generate_chinese_tags,
    create_db,
    file_to_db,
    load_all_from_db,
    build_faiss_index,
    load_vector_storage,
    search_similar_chunks,
    answer_question_with_prompt
)

file_to_db("game.txt")  # This step may take ~30 minutes with KeyBERT

docs = load_all_from_db()
build_faiss_index(docs)
```

### 4. Run the Chatbot (Streamlit UI)

The chatbot uses Streamlit to provide an interactive interface
If you are new to the Streamlit UI, running the code below may prompt you to enter your email or skip it

However, Jupyter Notebook does not allow direct input for this prompt

Therefore, I built the code below to help you automatically skip this step when using Jupyter Notebook

Launch it with:

```python
import subprocess
import os

subprocess.run(
    "echo '' | streamlit run src/app.py",
    shell = True
)

```

You can then ask any question related to the 337 games. The chatbot will retrieve relevant text chunks and generate answers using GPT-4.

If this block doesn’t work for you:

(1) Open your terminal

(2) Move to the src folder where app.py located at

(3) Enter: streamlit run app.py

Then it should be working!

### API Keys Required

- **OpenAI API Key**: Required for embedding and answering via GPT-4.
  - Add your key to `config.py` like this:
    ```python
    OPENAI_API_KEY = "your-api-key-here"
    ```

### Output Files

- `best_video_games.csv`: Cleaned game list with headquarters location
- `game.txt`: Full Wikipedia text of 337 games
- `game.db`: SQLite database storing chunked text and tags
- `faiss_index`: FAISS index file for semantic search

## Thank you very much for testing my project!
## Author: Sim Wang
